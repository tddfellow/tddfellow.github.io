<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Testing | That TDD Fellow | Tech Blog | Screencasts]]></title>
  <link href="http://www.tddfellow.com/blog/categories/testing/atom.xml" rel="self"/>
  <link href="http://www.tddfellow.com/"/>
  <updated>2017-03-20T19:37:38+01:00</updated>
  <id>http://www.tddfellow.com/</id>
  <author>
    <name><![CDATA[Oleksii Fedorov (waterlink)]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Build Your Own Testing Framework. Part 6: Test Suite Does Not Run All Tests!]]></title>
    <link href="http://www.tddfellow.com/blog/2017/03/13/build-your-own-testing-framework-part-6-test-suite-does-not-run-all-tests/"/>
    <updated>2017-03-13T23:13:28+01:00</updated>
    <id>http://www.tddfellow.com/blog/2017/03/13/build-your-own-testing-framework-part-6-test-suite-does-not-run-all-tests</id>
    <content type="html"><![CDATA[<p>Welcome back to the new issue of &ldquo;Build Your Own Testing Framework&rdquo; series! When trying to implement better formatting, we have discovered that some of our test suites do not run all tests! Today we are going to fix that, and we will make sure that such test suite will fail if it didn&rsquo;t execute all tests.</p>

<!-- more -->


<p>This article is the sixth one of the series “Build Your Own Testing Framework” so make sure to stick around for next parts! Find all posts of these series <a href="/blog/categories/build-your-own-testing-framework/">here</a>.</p>

<p>Shall we get started?</p>

<h2>Verify All Tests Run</h2>

<p>We will start from the <code>RunTestSuiteTest</code> and run the test suite with the single test. Then we are going to assert that for that the test with the name <code>testOk</code> has been reported as passing:</p>

<pre><code class="javascript">// test/RunTestSuiteTest.js

this.testItOutputsOkForThePassingTest = function () {
    runTestSuite(function (t) {
        this.testOk = function () {
            t.assertTrue(true);
        };
    }, {reporter: reporter});

    reporter.assertHasReportedPassingTest("testOk");
};
</code></pre>

<p>If we run this test suite, we can see that only one test executes!</p>

<pre><code>RunTestSuiteTest
    testItCallsAllTestMethods

Process finished with exit code 0
</code></pre>

<p>Oh, that is interesting. This test suite does not run. Upon investigating, it turns out, that <code>process.exit(0)</code> is being called during the <code>runTestSuite(...)</code> function run. That is because of the latest feature that we have implemented - &ldquo;exit with an appropriate exit code (zero for success, and one for failure).&rdquo; We should be able to fix that by providing the process spy in the options of the <code>runTestSuite</code> function that we are calling from the inside of the individual tests in the <code>RunTestSuiteTest</code> test suite. And we ought to alleviate this kind of mistake somehow - we need a mechanism that would alert us if not all tests have been run. Maybe something like <code>verifyAllTestsRun: true</code> option for the <code>runTestSuite</code>. For that let&rsquo;s write a test:</p>

<pre><code class="javascript">this.testVerifyAllTestsRun = function () {
    t.assertThrow("Expected all tests to run", function () {

        runTestSuite(function SuiteWithTwoTests(t) {

            this.testWithRunTestSuite = function () {
                runTestSuite(function (t) {
                    this.testOne = function () {};
                }, {reporter: reporter});
            };

            this.testThatShouldAlsoRun = function () {};

        }, {reporter: reporter,
            process: process,
            verifyAllTestsRun: true});

    });
};
</code></pre>

<p>That might be a bit complex at first. Let&rsquo;s take a closer look how this test is supposed to work:</p>

<ol>
<li>First of all, we do assert that there was an assertion failure about all tests required to run.</li>
<li>Inside of the action for this assertion we create and run the new test suite with two tests:

<ul>
<li>test with the <code>runTestSuite</code> without process spy provided</li>
<li>empty test that should also execute</li>
</ul>
</li>
</ol>


<p>If we run this test, it will pass. That is unexpected because we wanted it to fail. Apparently, most inner <code>runTestSuite</code> is doing <code>process.exit(0)</code>.</p>

<p>For that to work, we will need to be able to provide a hook into <code>process.exit(code)</code> function. For that, we would need to create a <code>SimpleProcess</code> class, that allows installation of such hooks. Let&rsquo;s test-drive it!</p>

<h3><code>process.exit</code> with hooks</h3>

<p>First, we should start from the normal behavior without any hooks:</p>

<pre><code class="javascript">// test/SimpleProcessTest.js
var runTestSuite = require("../src/TestingFramework");
var SimpleProcess = require("../src/TestingFramework").SimpleProcess;
var ProcessSpy = require("./ProcessSpy");

runTestSuite(function SimpleProcessTest(t) {
    this.testWithoutHooks = function () {
        var globalProcess = new ProcessSpy();
        var process = new SimpleProcess(globalProcess);

        process.exit(0);

        t.assertEqual(0, globalProcess.hasExitedWithCode);
    };
});
</code></pre>

<p>When running this test, we will get a failure about <code>SimpleProcess</code> being undefined. So let&rsquo;s define it:</p>

<pre><code class="javascript">// src/TestingFramework.js

// ...
// define the class itself
function SimpleProcess(globalProcess) {

}

// ..
// and don't forget to export it
module.exports.SimpleProcess = SimpleProcess;
</code></pre>

<p>If we run our test suite now, we will get an error <code>TypeError: process.exit is not a function</code>. To fix that failure we will have to define the <code>exit(code)</code> method on our newly created class <code>SimpleProcess</code>:</p>

<pre><code class="javascript">function SimpleProcess(globalProcess) {
    this.exit = function (code) {

    };
}
</code></pre>

<p>After doing that we will get an assertion failure <code>Error: Expected to equal 0, but got: null</code>, as expected. To make the test pass, it would be enough to call <code>globalProcess.exit(0)</code>:</p>

<pre><code class="javascript">this.exit = function (code) {
    globalProcess.exit(0);
};
</code></pre>

<p>If we run our test suite now, we will get no failures. That is great! Now, we can see that <code>globalProcess.exit(0)</code> is probably not exactly what we want to have there. We ought to pass the <code>code</code> parameter to the <code>exit</code> function. To test-drive this properly, we will have to triangulate, i.e.: add another test with the different value of the <code>code</code> parameter:</p>

<pre><code class="javascript">this.testWithoutHooks_andDifferentExitCode = function () {
    var globalProcess = new ProcessSpy();
    var process = new SimpleProcess(globalProcess);

    process.exit(1);

    t.assertEqual(1, globalProcess.hasExitedWithCode);
};
</code></pre>

<p>That fails as expected: <code>Error: Expected to equal 1, but got: 0</code>. To make it pass we can either write some weird &ldquo;if&rdquo; statement or we could pass the <code>code</code> parameter to the <code>globalProcess.exit</code> function. The second option is simpler. According to the third rule of test-driven development, we should go for it:</p>

<pre><code class="javascript">this.exit = function (code) {
    globalProcess.exit(code);
};
</code></pre>

<p>That change makes our test suite pass. We probably should refactor the test suite to reduce the level of the duplication by extracting common variables from the tests:</p>

<pre><code class="javascript">runTestSuite(function SimpleProcessTest(t) {
    var globalProcess = new ProcessSpy();
    var process = new SimpleProcess(globalProcess);

    this.testWithoutHooks = function () {
        process.exit(0);

        t.assertEqual(0, globalProcess.hasExitedWithCode);
    };

    this.testWithoutHooks_andDifferentExitCode = function () {
        process.exit(1);

        t.assertEqual(1, globalProcess.hasExitedWithCode);
    };
});
</code></pre>

<p>At that point, we should move on to tests for the hook installation functionality. Because right now we need only at most one hook we will not support multiple hooks at the same time - only one:</p>

<pre><code class="javascript">this.testCanInstallOneHook = function () {
    var aSpy = t.spy();

    process.installHook(aSpy);
    process.exit(0);

    aSpy.assertCalled();
};
</code></pre>

<p>When we run this test, it fails because <code>installHook</code> function is not defined: <code>TypeError: process.installHook is not a function</code>. So we should define it:</p>

<pre><code class="javascript">function SimpleProcess(globalProcess) {
    // ..
    this.installHook = function (aHook) {

    };
}
</code></pre>

<p>Upon running these tests, we get <code>Error: Expected to be called</code> because we didn&rsquo;t call this hook yet. The simplest way to make it pass is to just call the hook from the <code>installHook</code> function:</p>

<pre><code class="javascript">function SimpleProcess(globalProcess) {
    // ...
    this.installHook = function (aHook) {
        aHook();
    };
}
</code></pre>

<p>While that will make the tests pass it is not the behavior that we are after. To drive out the correct behavior, we ought to check that the function is being called only after <code>process.exit(..)</code>, not earlier. For that we will need to have a sanity-check assertion:</p>

<pre><code class="javascript">this.testCanInstallOneHook = function () {
    var aSpy = t.spy();

    process.installHook(aSpy);
    aSpy.assertNotCalled();

    process.exit(0);

    aSpy.assertCalled();
};
</code></pre>

<p>That fails as expected with the error <code>Error: Expected not to be called</code>. To make it pass we need to store the function in the variable and call it from the <code>process.exit(..)</code>:</p>

<pre><code class="javascript">function SimpleProcess(globalProcess) {
    var hook = null;

    this.exit = function (code) {
        if (hook !== null)
            hook();

        globalProcess.exit(code);
    };

    this.installHook = function (aHook) {
        hook = aHook;
    };
}
</code></pre>

<p>All the tests pass now! Finally, we want to be able to uninstall the hook, so let&rsquo;s write the test for it:</p>

<pre><code class="javascript">this.testCanUninstallTheHook = function () {
    var aSpy = t.spy();

    process.installHook(aSpy);
    process.uninstallHook();

    process.exit(0);

    aSpy.assertNotCalled();
};
</code></pre>

<p>To make it work it is enough to introduce this function and set <code>hook</code> variable back to <code>null</code> in it:</p>

<pre><code class="javascript">function SimpleProcess(globalProcess) {
    // ...
    this.uninstallHook = function () {
        hook = null;
    };
}
</code></pre>

<p>And all the tests will pass. Now we, also, want to replace the default value for the <code>options.process</code> option with the instance of <code>SimpleProcess</code> object. And all the tests should work as they were working before:</p>

<pre><code class="javascript">var simpleProcess = new SimpleProcess(global.process);

function TestSuiteRunContext(testSuiteConstructor, options) {
    // ...

    var process = options.process || simpleProcess;
    // instead of just "global.process"

    // ...
}
</code></pre>

<h3>Installing the &ldquo;verify all tests run&rdquo; hook</h3>

<p>Now, we can get back to our &ldquo;verify all tests run&rdquo; test. It still doesn&rsquo;t fail as expected, so we need to install the hook, count all tests, count tests that had already run and compare them in the hook:</p>

<pre><code class="javascript">function TestSuiteRunContext(testSuiteConstructor, options) {
    // ...
    var verifyAllTestsRun = options.verifyAllTestsRun || false;
    var testCount = 0;
    var testRun = 0;

    this.invoke = function () {
        if (verifyAllTestsRun)
            installVerifyAllTestsRunHook();  // &lt;---

        reportTestSuite();
        countAllTests();                     // &lt;---
        runAllTests();
        finishTestRun();
    };

    function installVerifyAllTestsRunHook() {
        simpleProcess.installHook(function () {
            if (testRun &lt; testCount) {
                throw new Error("Expected all tests to run");
            }
        });
    }

    // ...

    function countAllTests() {
        for (var testName in createTestSuite())
            if (testName.match(/^test/))
                testCount++;
    }

    // ...

    function handleTest(testName) {
        testRun++;                            // &lt;---
        reportTest(testName);
        runTest(createTestSuite(), testName);
    }
}
</code></pre>

<p>At this point, this throws an error <code>Expected all tests to run</code> and finishes the test fully without reaching our <code>assertThrow(..)</code> assertion. That happens because we catch this error in the function <code>runTest</code>, where we mark the test as failed, log the error and ignore the error object itself from there. One way to solve this problem is to have a particular error, that can propagate up the stack:</p>

<pre><code class="javascript">function installVerifyAllTestsRunHook() {
    simpleProcess.installHook(function () {
        if (testRun &lt; testCount) {
            var error = new Error("Expected all tests to run");
            error.bubbleUp = true;
            throw error;
        }
    });
}

// ...

function runTest(testSuite, testName) {
    try {
        testSuite[testName]();
    } catch (error) {
        if (error.bubbleUp) throw error;  // &lt;---
        if (!silenceFailures) console.log(error);
        status.markAsFailed();
    }
}
</code></pre>

<p>Now our current test is passing, and the next test is failing with the error <code>Expected all tests to run</code>. That happens because we have not uninstalled the hook as soon as it has triggered. Let&rsquo;s do that:</p>

<pre><code class="javascript">function installVerifyAllTestsRunHook() {
    simpleProcess.installHook(function () {
        if (testRun &lt; testCount) {
            simpleProcess.uninstallHook();   // &lt;---

            var error = new Error("Expected all tests to run");
            error.bubbleUp = true;
            throw error;
        }
    });
}
</code></pre>

<p>That makes the next test run, succeed and exit immediately after that with error code zero. Let&rsquo;s see what will happen if we put <code>verifyAllTestsRun: true</code> on the top test suite here:</p>

<pre><code class="javascript">runTestSuite(function RunTestSuiteTest(t) {
    // ...
}, {verifyAllTestsRun: true});
</code></pre>

<p>That doesn&rsquo;t work because we re-install different hook inside of this test and as soon as this test finishes, we uninstall it. So we have two ways out of this situation: allow multiple hooks, or move that single test to its own test suite file. I think the second options is much simpler. Also, we will add the test for the negative case, where all tests run correctly (when we provide proper process spy):</p>

<pre><code class="javascript">// test/VerifyAllTestsRunTest.js
var runTestSuite = require("../src/TestingFramework");
var ReporterSpy = require("./ReporterSpy");
var ProcessSpy = require("./ProcessSpy");

runTestSuite(function VerifyAllTestsRunTest(t) {
    var reporter = new ReporterSpy(t);
    var process = new ProcessSpy(t);

    this.testVerifyAllTestsRun = function () {
        t.assertThrow("Expected all tests to run", function () {

            runTestSuite(function SuiteWithTwoTests(t) {

                this.testWithRunTestSuite = function () {
                    runTestSuite(function (t) {
                        this.testOne = function () {};
                    }, {reporter: reporter});
                };

                this.testThatShouldAlsoRun = function () {};

            }, {reporter: reporter,
                process: process,
                verifyAllTestsRun: true});

        });
    };

    this.testVerifyAllTestsRun_withoutFailure = function () {
        t.assertNotThrow(function () {

            runTestSuite(function SuiteWithTwoTests(t) {

                this.testWithRunTestSuite = function () {
                    runTestSuite(function (t) {
                        this.testOne = function () {};
                    }, {reporter: reporter,
                        process: process});
                };

                this.testThatShouldAlsoRun = function () {};

            }, {reporter: reporter,
                process: process,
                verifyAllTestsRun: true});

        });
    };
});
</code></pre>

<p>And this new test suite passes as expected. Just to double-check that these tests verify anything, we can break them (change expected error message and change <code>assertNotThrow</code> to <code>assertThrow</code>) and see if there is a failure and if it looks as expected:</p>

<pre><code class="javascript">// was: t.assertThrow("some error", ...);
t.assertThrow("some error", function () {
    // ...
});
// =&gt; Error: Expected to equal some error,
//    but got: Expected all tests to run

// was: t.assertNotThrow(...);
t.assertThrow("some error", function () {
    // ...
});
// =&gt; Error: Expected to throw an error,
//    but nothing was thrown
</code></pre>

<p>And it fails as expected, which means that our refactored tests still work as they should.</p>

<blockquote><p>We have just applied a neat technique here: whenever we do a major refactoring in tests, we need to make sure they are still functioning correctly. For that, we break every single one of them (by changing the assertion or breaking the production code). Then we see if they fail as we expect them to. When they don&rsquo;t, we know that refactoring didn&rsquo;t quite work.</p></blockquote>

<h3>Fixing test suites to run all tests</h3>

<p>Now we can go back to the <code>RunTestSuiteTest</code> and see if it works as expected without that test. And it does: <code>Error: Expected all tests to run</code>. To fix that we need to provide a process spy in every inner call to <code>runTestSuite</code>. For that we will first extract <code>{reporter: reporter}</code> as a common variable of the test suite:</p>

<pre><code class="javascript">var options = {reporter: reporter};

// ...
// all the inner calls to "runTestSuite":
runTestSuite(function(t) {
    // ...
}, options);
</code></pre>

<p>And to make the error go away, we now can create a process spy and provide it through options:</p>

<pre><code class="javascript">var process = new ProcessSpy(t);
var options = {
    reporter: reporter,
    process: process
};
</code></pre>

<p>If we run tests now, they all pass. And we can see that they all execute. Now we just need to double-check that all tests, that have inner calls to <code>runTestSuite</code> have <code>verifyAllTestsRun</code> option enabled. The only other test suite is the <code>FailureTest</code>. Adding the option does not produce a failure because this test suite already uses process spy in all inner calls to <code>runTestSuite</code>.</p>

<h2>Conclusion</h2>

<p>Today we learned that it is tricky to work with <code>process.exit</code> or any function that can exit our program in the middle of the test. Such functions need to be mocked out completely inside of the tests. Also, we learned that it is possible to make sure we don&rsquo;t forget to do that. That is quite important because, if we do forget, everything runs smoothly, and we don&rsquo;t know that we made a mistake.</p>

<p>There is still a lot to go through. In a few next episodes we will:</p>

<ul>
<li>Report OK and FAIL for each test;</li>
<li>Output carefully formatted failures to the STDERR;</li>
<li>Enable our testing framework to run multiple test suite files at once;</li>
<li>Enable our testing framework to run in a browser (it is javascript after all).</li>
</ul>


<p>See you reading the next exciting article of the series: &ldquo;Formatting the Output&rdquo;!</p>

<h2>Thanks</h2>

<p>Thank you for reading, my dear reader. If you liked it, please share this article on social networks and follow me on Twitter: <a href="https://twitter.com/waterlink000">@waterlink000</a>.</p>

<p>If you have any questions or feedback for me, don’t hesitate to reach me out on Twitter: <a href="https://twitter.com/waterlink000">@waterlink000</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Understanding Legacy Code Using Explorative Test-Driven Development Technique]]></title>
    <link href="http://www.tddfellow.com/blog/2016/12/05/understanding-legacy-code-using-explorative-test-driven-development-technique/"/>
    <updated>2016-12-05T23:23:36+01:00</updated>
    <id>http://www.tddfellow.com/blog/2016/12/05/understanding-legacy-code-using-explorative-test-driven-development-technique</id>
    <content type="html"><![CDATA[<p>Today we are going to learn how to eliminate the fear of changing legacy code. We will learn how to confidently and in small iterations understand the legacy code better while increasing the test coverage in the process. While code examples are in Ruby programming language, the technique applied is language-agnostic.</p>

<!-- more -->


<p>For this article, we will need to define what Legacy Code means.</p>

<h2>Legacy Code</h2>

<p>Legacy code is challenging to understand when reading. Such code has no or close to no tests. Also, any legacy code brings the value to the business and customers.</p>

<p>Let&rsquo;s give an outline of what we will be going through today:</p>

<ul>
<li>We will define &ldquo;Knowledge&rdquo; and &ldquo;Mutation&rdquo; concepts in the context of the production code.</li>
<li>We will take a look into the relation between the production system and its test suite. While the connection from test suite to the production system is simple, the reverse connection is subtle and have some unusual and unexpected properties.</li>
<li>We will dismantle different test coverage metrics and outline the most valuable and useful one.</li>
<li>We will explore existing technique called Mutational Testing, that is simple to apply to the untested code to increase its test coverage.</li>
<li>We will introduce the technique called Explorative Test-Driven Development, that is an improvement of Mutational Testing method, which allows us to increase understanding of the legacy code in small steps - confidently and incrementally.</li>
<li>We will look at the example legacy code and apply Explorative TDD to it.</li>
<li>We will see the opportunities to use Explorative TDD technique outside the context of the Legacy Code.</li>
</ul>


<p>Shall we get the ball rolling?</p>

<h2>Knowledge in Production Code</h2>

<p>&ldquo;Knowledge in Production Code&rdquo; is any small bit of functionality that represents any part of the business rule or underlying infrastructure rule. Example bits of knowledge in production code:</p>

<ul>
<li>a variable assignment (or binding): <code>a_variable = ...</code>,</li>
<li>the presence of the <code>if</code> statement: <code>if ... end</code>,</li>
<li>an <code>if</code> condition: <code>if has_certain_property()</code>,</li>
<li>an <code>if</code> body: <code>if ...  do_something_interesting  end</code>,</li>
<li>the presence of the <code>else</code> clause: <code>if ... else ... end</code>,</li>
<li>an <code>else</code> body: <code>if ... else  do_something_different  end</code>,</li>
<li>function (or method) call: <code>a_function(arguments)</code>, <code>receiver.a_method(arguments)</code>,</li>
<li>every argument of the function (or method) call (including receiver),</li>
<li>a constant: <code>42</code>,</li>
<li>the fact that function (or method) returns early: <code>if ...  return 42  end</code>,</li>
<li>what the function (or method) returns,</li>
<li>the presence of the iteration: <code>...each do |x| ... end</code>,</li>
<li>what we iterate through: <code>list.each do ...</code>,</li>
<li>and how we are iterating: <code>...each do |x|  do_something_with(x)  end</code>,</li>
<li>and so on.</li>
</ul>


<p>I think the idea &ldquo;Knowledge in Production Code&rdquo; should be more or less precise. More interesting is what we can do with knowledge in our system: we can re-organize knowledge differently keeping all the behaviors of the system - everyone calls this Refactoring nowadays; or do the opposite: change bits of knowledge without modifying the structure of the code - we will call one such change a Mutation:</p>

<h2>Mutation</h2>

<p>Mutation - granular change of the knowledge in the system that changes the behavior of the application. Let&rsquo;s take a look at the simple example:</p>

<pre><code class="ruby">if cell_is_alive
  do_this
else
  do_some_other_thing
end
</code></pre>

<p>This code is maybe a part of some cell organism simulation (like Game Of Life or similar). Let&rsquo;s see which different mutations can be applied here:</p>

<ul>
<li>change the <code>if</code> condition always to be <code>true</code>: <code>if true ...</code>,</li>
<li>change the <code>if</code> condition always to be <code>false</code>: <code>if false ...</code>,</li>
<li>invert the <code>if</code> condition: <code>if !cell_is_alive</code>,</li>
<li>commenting out the <code>if</code> body: <code># do_this</code>,</li>
<li>commenting out the <code>else</code> body: <code># do_some_other_thing</code>.</li>
</ul>


<p>With that done, let&rsquo;s take a look at how production code and its test suite relate to each other.</p>

<h2>Code and Test Suite Relationship</h2>

<p>So, how does the test suite affect production code? First, it makes sure the production code is correct. Also, good test suite enables quick and ruthless refactoring by eliminating (or minimizing) the risks of breaking it. Well-crafted test suite gives us the power and courage to introduce changes. Also, test code always couples to the production code it is testing in one way or another.</p>

<p>Okay, how does the production system affect its test suite? As tests couple to the production code they test, the changes in production system may cause ripple effects on its test suite. Practically speaking, a mutation should always lead to a test failure if the test suite is good enough because its test suite should verify every tiny bit of knowledge in the production code (except, maybe, some configuration).</p>

<p>Such knowledge change is an act of assertion about the presence of the test. When information is covered by test suite well, there should be a test failure. If, after the introduction of the mutation, there is no test failure, this is a failed assertion about test presence or correctness. So one might say:</p>

<blockquote><p>Knowledge Change is a Test for the Test</p></blockquote>

<p>That is a fascinating idea since it implies we can use production code can as a test suite for its test suite, which may enable TDD-like iterative development of the test suite that does not exist.</p>

<p>So far, we have covered the idea of knowledge in the production code, explored ways of modifying this information in a way that changes the behavior - we call it a mutation, and also we explored the mirror-like relation between production code and its test suite. We have still much ground to cover, let&rsquo;s dive in:</p>

<h2>Most Useful Coverage Metric</h2>

<p>There is a few well-known test coverage metrics that are being used quite often by software engineering teams, such as:</p>

<ul>
<li>Line coverage, and</li>
<li>Branch coverage.</li>
</ul>


<p>There is another one, called Path coverage - it is about coverage of all possible code paths in the system, which quickly becomes impractical as the application size grows because of the exponential growth of the amount of these different code paths.</p>

<p>Line coverage and Branch coverage (also, path coverage) all share one major problem - covered line/branch/path does not mean test suite verifies it - only executes it. Great example: remove all the assertions from your tests and the coverage metric will stay the same.</p>

<p>So, what if we could introduce all possible and sane mutations to our code and count how much of them cause test failure? - We will get the knowledge coverage metric. Another name for it is Test Semantic Stability, and it can range from 0% to 100%. Even 100% line/path coverage can easily yield 0% Test Semantic Stability. This metric proves that code is, indeed well-tested and verified (although, it does not say anything about tests&#8217; design and cleanliness): make one assertion incorrect, or not precise enough and the metric will go down by a few mutations.</p>

<p>That makes Test Semantic Stability the most useful coverage metric.</p>

<p>So, how do we check if our test(s) cover well some bit of knowledge in the system? We break it! - Introduce a tiny granular breaking change to that bit of knowledge. The test suite should fail. If it does not - information is not covered well enough. That leads us to the technique that allows us to keep Semantic Test Stability up high:</p>

<h2>Mutational Testing</h2>

<ol>
<li>Narrow the scope of work to a single granular piece of knowledge.</li>
<li>Break this knowledge (introduce simple granular breaking change - mutation).</li>
<li>Make sure there is a test suite failure.</li>
<li>Restore the knowledge to its original state (CTRL+Z, ideally).</li>
</ol>


<p>Let&rsquo;s see it in action:</p>

<pre><code class="ruby">if cell_is_alive
  do_this
else
  do_some_other_thing
end
</code></pre>

<p>First, we need to narrow our scope to a single bit of knowledge. For example, the <code>if</code> condition: <code>if cell_is_alive</code>. Then we need to introduce the mutation <code>if true,</code> and we need to make sure that there is a test failure. Let&rsquo;s run the test suite:</p>

<pre><code>$ rake test
....

Finished in 0.02343 seconds (files took 0.11584 seconds to load)
4 examples, 0 failures
</code></pre>

<p>Oh no! It did not fail anywhere! That means that we have a &ldquo;failing test&rdquo; for our test suite. In this case, we need to add the test for the negative case:</p>

<pre><code class="ruby">cell_is_alive = false
expect(did_some_other_thing).to eq(true)
</code></pre>

<p>When we run the test suite:</p>

<pre><code>$ rake test
....F

Finished in 0.02343 seconds (files took 0.11584 seconds to load)
5 examples, 1 failure
</code></pre>

<p>It fails! Great - that means that our test for the test suite is passing now. As the last step of this mutational testing iteration we have to return the code to its original state:</p>

<pre><code class="ruby">if cell_is_alive
  do_this
else
  do_some_other_thing
end
</code></pre>

<p>After doing this, our tests should pass!:</p>

<pre><code>$ rake test
.....

Finished in 0.02343 seconds (files took 0.11584 seconds to load)
5 examples, 0 failures
</code></pre>

<p>They do. That concludes one iteration of the mutational testing. Usually, to accomplish any useful behavior we would like to combine many bits of knowledge. If we want to understand better how the system works, we need to focus on groups of bits of knowledge. This is what Explorative TDD technique is about:</p>

<h2>Explorative Test-Driven Development</h2>

<p>The technique used to increase our understanding of the Legacy Code while enhancing its Test Semantic Stability (the most useful coverage metric). The process roughly looks like that:</p>

<ol>
<li>Narrow scope to some manageable knowledge and isolate it (manageable knowledge = method/function/class/module).</li>
<li>Read, try to understand, pick a granular piece of knowledge, and make an assumption to which behavior it contributes and how.</li>
<li>Write a test to verify this assumption.</li>
<li>Make sure test passes (by altering the assumption or fixing production code (bugs)). PS: be careful with bugs, since they might be weird behaviors that are bringing someone tremendous value. When finding one of these, consult with stakeholders if that is a bug or a feature.</li>
<li>Apply Mutational Testing to each related granular piece of knowledge to verify that the understanding (and the test) is correct (this may introduce more tests).</li>
<li>Go back to 2</li>
</ol>


<p>At this point, a nice example would help understand that technique:</p>

<h2>Step-by-Step Example</h2>

<p>Let&rsquo;s imagine that we have some legacy system, that is a social network and allows for users to receive notifications on things that happened. You need to change slightly what &ldquo;Followed&rdquo; notification means. The code looks like this, and it does not have any tests:</p>

<pre><code class="ruby">class User
  def notifications
    notifications = Database
      .where("notifications") do |x|
        (x[1][0] == "followed_notification" &amp;&amp; x[1][2] == id.to_s) ||
        (x[1][0] == "favorited_notification" &amp;&amp; StatusUpdate.find(x[1][2].to_i).owner_id == id) ||
        (x[1][0] == "replied_notification" &amp;&amp; StatusUpdate.find(x[1][2].to_i).owner_id == id) ||
        (x[1][0] == "reposted_notification" &amp;&amp; StatusUpdate.find(x[1][2].to_i).owner_id == id)
      end.map do |row|
        id, values = row
        kind = values[0]

        if kind == "followed_notification"
          {
            kind: kind,
            follower: User.find(values[1].to_i),
            user: User.find(values[2].to_i),
          }
        elsif kind == "favorited_notification"
          {
            kind: kind,
            favoriter: User.find(values[1].to_i),
            status_update: StatusUpdate.find(values[2].to_i),
          }
        elsif kind == "replied_notification"
          {
            kind: kind,
            sender: User.find(values[1].to_i),
            status_update: StatusUpdate.find(values[2].to_i),
            reply: StatusUpdate.find(values[3].to_i),
          }
        elsif kind == "reposted_notification"
          {
            kind: kind,
            reposter: User.find(values[1].to_i),
            status_update: StatusUpdate.find(values[2].to_i),
          }
        end
      end

    Analytics.tag({name: "fetch_notifications", count: notifications.count})
    notifications
  end
end
</code></pre>

<h3>Narrow &amp; Isolate</h3>

<p>The first step is to isolate this code and make it testable. For this we need to find a low-risk way to refactor all dependencies that this code has:</p>

<ul>
<li><code>Database.where</code>,</li>
<li><code>StatusUpdate.find</code>,</li>
<li><code>User.find</code>, and</li>
<li><code>Analytics.tag</code>.</li>
</ul>


<p>We can promote these things to the following roles:</p>

<ul>
<li><code>Database.where</code> => <code>@table_reader.where</code>,</li>
<li><code>StatusUpdate.find</code> => <code>@status_update_finder.where</code>,</li>
<li><code>User.find</code> => <code>@user_finder.find</code>, and</li>
<li><code>Analytics.tag</code> => <code>@event_tagger.tag</code>.</li>
</ul>


<p>We should be able to have these default to their original values and also allow to substitute different implementation from the test. Also, it is helpful to pull out this method into the clean environment, where accessing a dependency, without us substituting it - is not possible, for example in a separate code-base, so that we can write a test &ldquo;it works&rdquo; and see what fails. The first failure is, of course, all our referenced classes are missing. Let&rsquo;s define all of them without any implementation and make them fail at runtime if we ever call them from our testing environment:</p>

<pre><code class="ruby">class Database
  def self.where(table_name)
    fail "Database:nope"
  end
end

class Analytics
  def self.tag(event)
    fail "Analytics:nope"
  end
end

class StatusUpdate
  def self.find(id)
    fail "StatusUpdate:nope"
  end
end

class User
  # .. def notifications ..

  def self.find(id)
    fail "User:nope"
  end
end
</code></pre>

<p>In our tests, we need to implement our substitutes. For now, they all should be just simple double/stubs:</p>

<pre><code class="ruby">class FakeTableReader
  def where(table_name, &amp;filter)
    [[nil, ["favorited_notification"]]]
  end
end

class FakeEventTagger
  def tag(event)

  end
end

class FakeUserFinder
  def find(id)
    User.new
  end
end

class FakeStatusUpdateFinder
  def find(id)
    StatusUpdate.new
  end
end
</code></pre>

<p>Then, we should write the simplest test, that sets up the stage and substitutes all the collaborators and runs the function under the test (no assertion, we are just verifying that we indeed replaced everything right):</p>

<pre><code class="ruby">it "works" do
  fake_table_reader = FakeTableReader.new
  fake_event_tagger = FakeEventTagger.new
  fake_user_finder = FakeUserFinder.new
  fake_status_update_finder = FakeStatusUpdateFinder.new

  user = User.new
             .with_table_reader(fake_table_reader)
             .with_event_tagger(fake_event_tagger)
             .with_user_finder(fake_user_finder)
             .with_status_update_finder(fake_status_update_finder)

  user.notifications
end
</code></pre>

<p>Since we have not defined all the <code>with_*</code> methods yet, let&rsquo;s define them now and also define getters for particular instance variables (properties):</p>

<pre><code class="ruby">class User
  # ...

  def table_reader
    @table_reader ||= Database
  end

  def event_tagger
    @event_tagger ||= Analytics
  end

  def user_finder
    @user_finder || User
  end

  def status_update_finder
    @status_update_finder || StatusUpdate
  end

  def with_table_reader(table_reader)
    @table_reader = table_reader
    self
  end

  def with_event_tagger(event_tagger)
    @event_tagger = event_tagger
    self
  end

  def with_user_finder(user_finder)
    @user_finder = user_finder
    self
  end

  def with_status_update_finder(status_update_finder)
    @status_update_finder = status_update_finder
    self
  end
end
</code></pre>

<p>If we run our test, it should fail with <code>RuntimeError: Database:nope</code> in here:</p>

<pre><code class="ruby">def notifications
  notifications = Database            # &lt;&lt;&lt;&lt;&lt;&lt;
    .where("notifications") do |x|
</code></pre>

<p>To fix that, we will need to replace <code>Database</code> with <code>table_reader</code> getter. That will correct the current error, and we will get the next one: <code>RuntimeError User:nope</code>. Following all these failures and replacing direct dependencies with getters we will finally get a Green Bar (passing the test). Our function under the test will look like that:</p>

<pre><code class="ruby">class User
  def notifications
    notifications = table_reader
      .where("notifications") do |x|
        (x[1][0] == "followed_notification" &amp;&amp; x[1][2] == id.to_s) ||
            (x[1][0] == "favorited_notification" &amp;&amp; status_update_finder.find(x[1][2].to_i).owner_id == id) ||
            (x[1][0] == "replied_notification" &amp;&amp; status_update_finder.find(x[1][2].to_i).owner_id == id) ||
            (x[1][0] == "reposted_notification" &amp;&amp; status_update_finder.find(x[1][2].to_i).owner_id == id)
      end.map do |row|
        id, values = row
        kind = values[0]

        if kind == "followed_notification"
          {
              kind: kind,
              follower: user_finder.find(values[1].to_i),
              user: user_finder.find(values[2].to_i),
          }
        elsif kind == "favorited_notification"
          {
              kind: kind,
              favoriter: user_finder.find(values[1].to_i),
              status_update: status_update_finder.find(values[2].to_i),
          }
        elsif kind == "replied_notification"
          {
              kind: kind,
              sender: user_finder.find(values[1].to_i),
              status_update: status_update_finder.find(values[2].to_i),
              reply: status_update_finder.find(values[3].to_i),
          }
        elsif kind == "reposted_notification"
          {
              kind: kind,
              reposter: user_finder.find(values[1].to_i),
              status_update: status_update_finder.find(values[2].to_i),
          }
        end
      end

    event_tagger.tag({name: "fetch_notifications", count: notifications.count})
    notifications
  end

  # ...
end
</code></pre>

<p>Structure and logic of the function did not change at all, but now all the dependencies are injectable and can be used to test it nicely. That concludes the first step - narrow &amp; isolate. Now it is time to select a group of knowledge bits that we would like to cover with tests. Since we want to change how <code>followed_notification</code> is behaving, we might as well start checking there.</p>

<p><div class="v2-subscribe--inline">
  




  


<div class="mc_embed_signup">
  <form action="//tddfellow.us14.list-manage.com/subscribe/post?u=535a10a8c0274c9a7ebac4f34&amp;id=7f9f94015a" method="post" class="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div class="mc_embed_signup_scroll">
      <h3>Want more articles like this delivered to your inbox?</h3>
      <div class="mc-field-group">
        <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
        <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_535a10a8c0274c9a7ebac4f34_7f9f94015a" tabindex="-1" value=""></div>
        <input type="email" value="" name="EMAIL" class="required email mce-EMAIL" placeholder="Enter your email">
        <input type="submit" value="Subscribe" name="subscribe" class="button mc-embedded-subscribe">
      </div>
      <div class="">
        <em>(we respect your privacy, unsubscribe at any time)</em>
      </div>
    </div>
  </form>
</div>


</div>
</p>

<h3>Trying to Understand &amp; Writing 1st Test</h3>

<p>The group of knowledge bits that are related to <code>followed_notification</code> looks like this:</p>

<pre><code class="ruby">    notifications = table_reader
      .where("notifications") do |x|
        (x[1][0] == "followed_notification" &amp;&amp; x[1][2] == id.to_s) ||
        # ...
      end.map do |row|
        id, values = row
        kind = values[0]

        if kind == "followed_notification"
          {
              kind: kind,
              follower: user_finder.find(values[1].to_i),
              user: user_finder.find(values[2].to_i),
          }
        elsif #...
          # ...
        end
      end

    # ...
    notifications
</code></pre>

<p>Now we want to write a test. At the first thought, something like:</p>

<pre><code class="ruby">it "obtains followed notifications for the user" do
  # first create a user with all fakes (extracted to a helper method)
  user = create_user_with_fakes

  # then instruct our table reader fake to return prepared data
  fake_table_reader
      .insert("notifications",
              [1001, ["followed_notification", 2001, 3001]])

  # and expect that we have exactly one notification
  expect(user.notifications.count).to eq(1)
end

def create_user_with_fakes
  User.new(567)
      .with_table_reader(fake_table_reader)
      .with_event_tagger(fake_event_tagger)
      .with_user_finder(fake_user_finder)
      .with_status_update_finder(fake_status_update_finder)
end

class FakeTableReader
  def insert(table_name, row)
    tables(table_name) &lt;&lt; row
  end

  def tables(table_name)
    @tables ||= {}
    @tables[table_name] ||= []
  end

  def where(table_name, &amp;filter)
    tables(table_name).select(&amp;filter)
  end
end
</code></pre>

<h3>Making It Pass</h3>

<p>This test fails right away - we don&rsquo;t have any notifications. This is strange. Let&rsquo;s take a closer look on the filtering that we are doing:</p>

<pre><code class="ruby">(x[1][0] == "followed_notification" &amp;&amp; x[1][2] == id.to_s) ||
</code></pre>

<p>I believe, we have satisfied the first part of this condition, but not the second one. The user id is not the same as the 3rd element of this row. Let&rsquo;s make them same:</p>

<pre><code class="ruby">fake_table_reader
    .insert("notifications",
            [1001, ["followed_notification", 2001, 567]])
                                               # ^ here ^
</code></pre>

<p>This fails again! This code just keeps proving our assumptions wrong. I think we need to take a careful look at that <code>it.to_s</code>. <code>.to_s</code> is a conversion to string, so the foreign key is stored as a string (who could have thought?). Let&rsquo;s try to make it work:</p>

<pre><code class="ruby">fake_table_reader
    .insert("notifications",
            [1001, ["followed_notification", 2001, "567"]])
                                                # ^ here ^
</code></pre>

<h3>Applying Mutational Testing</h3>

<p>If we run our tests, they pass! Great, now we know that this function is capable of obtaining some followed notifications. Of course, our coverage right now is super small. Let&rsquo;s apply mutational testing to it. We should start from the condition:</p>

<pre><code class="ruby">(x[1][0] == "followed_notification" &amp;&amp; x[1][2] == id.to_s) ||
</code></pre>

<p>First, let&rsquo;s replace the whole thing with <code>false</code>:</p>

<pre><code class="ruby">false ||
</code></pre>

<p>The test fails - mutant does not survive - our tests are covering for this mutation. Let&rsquo;s try another one: replace the whole thing with <code>true</code>:</p>

<pre><code class="ruby">true ||
</code></pre>

<p>Our tests pass - mutant survives - this is a failing test for our tests. In this case, it is reasonable to write a new test for a case, when the full filtering expression should yield <code>false</code> - when we have notifications of an invalid kind:</p>

<pre><code class="ruby">it "ignores notifications of an invalid kind" do
  user = create_user_with_fakes

  fake_table_reader
      .insert("notifications",
              [1001, ["invalid", 2001, "567"]])

  expect(user.notifications.count).to eq(0)
end
</code></pre>

<p>As a result, we should not get any notifications. After running, we see that our test fail. Great! This mutant no longer survives. Let&rsquo;s see if our tests will pass when we undo the mutation:</p>

<pre><code class="ruby">(x[1][0] == "followed_notification" &amp;&amp; x[1][2] == id.to_s) ||
</code></pre>

<p>And they all pass! Next mutation is inverting the whole condition:</p>

<pre><code class="ruby">! (x[1][0] == "followed_notification" &amp;&amp; x[1][2] == id.to_s) ||
</code></pre>

<p>All our tests are RED. Which means that this mutant does not survive and the test for our test is green. Now, we should dig deeper into the parts of the condition itself:</p>

<ul>
<li><code>x[1][0] == "followed_notification"</code>: replacing with <code>true</code>, <code>false</code>, and inverting it; also, changing numeric and string constants; These all changes did not produce any surviving mutants, so we do not need to introduce new tests.</li>
<li><code>x[1][2] == id.to_s</code>: replacing with <code>true</code>, <code>false</code> and inverting it; also, changing numeric constants.</li>
</ul>


<p>Replacing <code>x[1][2] == id.to_s</code> with <code>true</code>, apparently, leaves all our tests passing - a mutant that survives - a failing test for our test suite. It is time to add this test - when we have notifications of some different user:</p>

<pre><code class="ruby">it "ignores notifications of different user" do
  user = create_user_with_fakes

  fake_table_reader
      .insert("notifications",
              [1001, ["followed_notification", 2001, "other user"]])
                                                   # ^   here   ^

  expect(user.notifications.count).to eq(0)
end
</code></pre>

<p>As you can see, having a record with the different user id (in this case, even nonsensical user id) makes our test fail, which means that this mutant no longer survives. Let&rsquo;s see if undoing the mutation will turn our tests GREEN:</p>

<pre><code class="ruby">(... &amp;&amp; x[1][2] == id.to_s) ||
</code></pre>

<p>All our tests pass again. I think we have finished testing the condition in the filter. I would not touch the conditions that are related to different kinds of notifications, as we want to introduce changes only to &ldquo;Followed&rdquo; notifications. So we can dig further into the logic of our group of knowledge bits:</p>

<pre><code class="ruby">id, values = row
kind = values[0]

if kind == "followed_notification"
  {
      kind: kind,
      follower: user_finder.find(values[1].to_i),
      user: user_finder.find(values[2].to_i),
  }
elsif #...
  # ...
end
</code></pre>

<p>So, we can see that we split the row into its <code>id</code> and all the other values of the notification record. Apparently, the first value is responsible for the kind, where we are switching on it to construct correct object (in this case just a lump of data - hash map). So let&rsquo;s try to mutate the numeric constant in <code>kind = values[0]</code>:</p>

<pre><code class="ruby">kind = values[1]
          #  ^^^
</code></pre>

<p>All our tests still pass. That is a failing test for our test suite. We ought to write a new test now. Where we should verify that it constructs correct lumps of data:</p>

<pre><code class="ruby">it "constructs correct followed notification" do
  user = create_user_with_fakes

  fake_table_reader
      .insert("notifications",
              [1001, ["followed_notification", 2001, "567"]])

  expect(user.notifications[0][:kind]).to eq("followed_notification")
end
</code></pre>

<p>This test fails, because our <code>user.notifications[0]</code> Is <code>nil</code>, because none of <code>if</code> or <code>elsif</code> matched the <code>kind</code> variable and in Ruby, by default any function returns a <code>nil</code> value. This failing test means that we no longer have surviving mutant and let&rsquo;s see if undoing that mutation will make our tests pass:</p>

<pre><code class="ruby">kind = values[0]
          #  ^^^
</code></pre>

<p>It does, all our tests are green now. We should continue like this until we understand code enough and have enough confidence in our tests so that we can make our desired change to the system. When we think we have finished, we should integrate isolated code back to the legacy system, leaving all the fakes and injection capabilities in place. We were separating this code only to make sure, that we are not calling any dependencies on accident (while they just work silently). While integrating it back we, of course, get rid of <code>fail "NAME:nope"</code> implementations of collaborators. With such approach, integrating the code back should be as simple as copy-pasting the test suite code and production code (function under the test, and injecting facilities) without copying always-failing collaborators.</p>

<p>We will have to wrap up the example, and if you, my reader, would like to continue applying Explorative TDD to this code, you can find the code here: <a href="https://github.com/waterlink/explorative-tdd-blog-post">https://github.com/waterlink/explorative-tdd-blog-post</a> (specifically, <code>spec/user_spec.rb</code>). The function originates from this example project: <a href="https://github.com/waterlink/lemon">https://github.com/waterlink/lemon</a></p>

<h2>Can Explorative TDD Help Me Outside of Legacy Code?</h2>

<p>The answer is yes! I use Explorative TDD (as well as mutational testing) in following cases:</p>

<ul>
<li>During big refactorings, such as Extract class/module/package. The technique helps you quickly understand which tests have to be moved as well to the new test suite (only if you want to transfer them).</li>
<li>When refactoring tests. The technique helps you to verify if your tests are still working as they are intended to and if they are still semantically stable (they catch a majority of mutants).</li>
<li>To measure rigidity of test-to-code coupling. If a single mutation leads to half of your test suite failing (even irrelevant tests) - tests need refactoring.</li>
</ul>


<h2>Bottom Line</h2>

<p>Today we have learned about concepts like &ldquo;Knowledge in production code&rdquo; and &ldquo;Mutation.&rdquo; Also, we learned what Test Semantic Stability is the best code coverage metric. We have seen Mutational Testing and Explorative TDD techniques at work. We could start applying these techniques (after some practice) to stop fearing the legacy code and just handle it as some tedious routine operation.</p>

<h2>Thanks</h2>

<p>Thank you for reading, my dear reader. If you liked it, please share this article on social networks and follow me on twitter: <a href="https://twitter.com/waterlink000">@waterlink000</a>.</p>

<p>If you have any questions or feedback for me, don’t hesitate to reach me out on Twitter: <a href="https://twitter.com/waterlink000">@waterlink000</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Build Your Own Testing Framework. Part 5]]></title>
    <link href="http://www.tddfellow.com/blog/2016/11/13/build-your-own-testing-framework-part-5/"/>
    <updated>2016-11-13T12:50:51+01:00</updated>
    <id>http://www.tddfellow.com/blog/2016/11/13/build-your-own-testing-framework-part-5</id>
    <content type="html"><![CDATA[<p>Welcome back to the new issue of &ldquo;Build Your Own Testing Framework&rdquo; series! Did you notice, that out testing framework quits on the first failure? It probably should run all tests, collect all failures and present them nicely. This is what we are going to accomplish today:</p>

<ul>
<li>Make sure all tests run even when there is a failure.</li>
<li>Make sure exit code is correct.</li>
</ul>


<!-- more -->


<p>This article is the fifth one of the series “Build Your Own Testing Framework” so make sure to stick around for next parts! Find all posts of these series can <a href="/blog/categories/build-your-own-testing-framework/">here</a>.</p>

<p>Shall we get started?</p>

<h2>Catch and report a test failure</h2>

<p>Our test suite should no longer bubble up any exceptions. We can achieve that by making an appropriate assertion. And also we should verify that other tests execute after the failure:</p>

<pre><code class="javascript">runTestSuite(function FailureTest(t) {
    this.testItDoesNotBubbleUpExceptions = function () {
        var aSpy = t.spy();

        t.assertNotThrow(function () {
            runTestSuite(function (t) {
                this.testFailure = function () {
                    t.assertTrue(false);
                };

                this.testSomething = aSpy;
            });
        });

        aSpy.assertCalled();
    };
});
</code></pre>

<p>As expected, this fails with an appropriate error <code>Error: Expected not to throw error, but thrown 'Expected to be true, but got false'</code> indicating that we are bubbling up all errors at the moment. Also, notice how the execution of the whole test suite stops at that point, and it just exits the program with error code <code>1</code>. A simple <code>try .. catch</code> block will fix the issue:</p>

<pre><code class="javascript">// in runTestSuite function
    for (var testName in testSuitePrototype) {
        if (testName.match(/^test/)) {
            reporter.reportTest(testName);
            var testSuite = createTestSuite(testSuiteConstructor);

            try {
                testSuite[testName]();
            } catch (error) {
                // do nothing, for now
            }
        }
    }
</code></pre>

<p>All tests now run successfully. This code is starting to become unreadable, so it is a good point to refactor. We will:</p>

<ul>
<li>Extract whole <code>try .. catch</code> as a function <code>runTest</code>. Its current responsibility is only to run the test and ignore any failure;</li>
<li>Extract contents of <code>if</code> statement that matches the test name as a function <code>handleTest</code>. Its responsibility is to report the test, create a fresh testSuite and kick off <code>runTest</code>;</li>
<li>Extract the whole <code>for</code> statement as <code>runAllTests</code>.</li>
</ul>


<p>Here is the final snippet of code:</p>

<pre><code class="javascript">function runTest(testSuite, testName) {
    try {
        testSuite[testName]();
    } catch (error) {
        // do nothing, for now
    }
}

function handleTest(reporter, testName, testSuiteConstructor) {
    reporter.reportTest(testName);
    runTest(createTestSuite(testSuiteConstructor), testName);
}

function runAllTests(reporter, testSuitePrototype, testSuiteConstructor) {
    for (var testName in testSuitePrototype) {
        if (testName.match(/^test/)) {
            handleTest(reporter, testName, testSuiteConstructor);
        }
    }
}

function runTestSuite(testSuiteConstructor, options) {
    options = options || {};
    var reporter = options.reporter || new SimpleReporter();

    var testSuitePrototype = createTestSuite(testSuiteConstructor);

    reporter.reportTestSuite(
        getTestSuiteName(testSuiteConstructor, testSuitePrototype)
    );

    runAllTests(reporter, testSuitePrototype, testSuiteConstructor);
}
</code></pre>

<h2>Exit with code 1</h2>

<p>Now, when at least one test fails in a suite of tests, the whole suite should fail (after running the rest of its tests). And the indicator of such failure should be an exit code of the process. Let&rsquo;s write a test:</p>

<pre><code class="javascript">runTestSuite(function FailureTest(t) {
    // ...

    this.testItExitsWithProcessCodeOne = function () {
        var processSpy = new ProcessSpy();

        runTestSuite(function (t) {
            this.testFailure = function () {
                t.assertTrue(false);
            };
        }, {process: processSpy});

        t.assertEqual(1, processSpy.hasExitedWithCode);
    };
});
</code></pre>

<p>As you might guess, we will need another object. It will be responsible for interaction with our process, i.e.: something that we can ask to &ldquo;exit with code 1.&rdquo; Because we can not ask our process to exit within the test run, we will have to create a spy. And we shall test-drive its functionality. There is something interesting that we should worry about before that - our test suite is passing currently.. but it shouldn&rsquo;t be!</p>

<p>Let&rsquo;s step back and think what just happened: clearly, we are writing the test, that can not possibly pass because we do not have <code>ProcessSpy</code> yet. So we are expecting a failure - we are expecting a thrown exception. That expectation is an important part of test-driven development: at all times we expect a very specific failure or we expect our tests to pass; if we do not receive a failure when expected and receive an unexpected failure, we should stop right there and think which part of our thinking and our assumptions is incorrect.</p>

<p><div class="v2-subscribe--inline">
  
  
  
  
  

<div class="mc_embed_signup">
  <form action="//tddfellow.us14.list-manage.com/subscribe/post?u=535a10a8c0274c9a7ebac4f34&amp;id=6b61a409a5" method="post" class="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div class="mc_embed_signup_scroll">
      <h3>Would You Like to Watch a Screencast with Me Implementing This?</h3>
      <div class="mc-field-group">
        <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
        <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_535a10a8c0274c9a7ebac4f34_6b61a409a5" tabindex="-1" value=""></div>
        <input type="email" value="" name="EMAIL" class="required email mce-EMAIL" placeholder="Enter your email">
        <input type="submit" value="Claim video" name="subscribe" class="button mc-embedded-subscribe">
      </div>
      <div class="">
        <em>(we respect your privacy, unsubscribe at any time)</em>
      </div>
    </div>
  </form>
</div>

</div>
</p>

<p>Right now, tests do not fail, because we are ignoring all exceptions in our <code>try .. catch</code> that we introduced a couple of minutes ago. If we want to see failures again, let&rsquo;s modify <code>catch</code> block to just log all errors it receives:</p>

<pre><code class="javascript">function runTest(testSuite, testName) {
    try {
        testSuite[testName]();
    } catch (error) {
        console.log(error);
    }
}
</code></pre>

<p>Now our test suite outputs an expected error: <code>ReferenceError: ProcessSpy is not defined</code>. Also, it outputs some other failures that happen in our nested <code>runTestSuite</code> calls - we should fix them by providing <code>silenceFailures</code> option for nested <code>runTestSuite</code> call. We can focus now on the <code>ProcessSpy</code> failure and test-drive it:</p>

<pre><code class="javascript">runTestSuite(function ProcessSpy_BehaviorTest(t) {
    var processSpy = new ProcessSpy();
});
// =&gt; ReferenceError: ProcessSpy is not defined

function ProcessSpy() {}
// =&gt; PASS

    this.testHasExitedWithCode_initiallyIsNull = function () {
        t.assertEqual(null, processSpy.hasExitedWithCode);
    };
// =&gt; Error: Expected to equal null, but got: undefined

function ProcessSpy() {
    this.hasExitedWithCode = null;
}
// =&gt; PASS

    this.testHasExitedWithCode_isZero_afterExitZeroCall = function () {
        processSpy.exit(0);
        t.assertEqual(0, processSpy.hasExitedWithCode);
    };
// =&gt; TypeError: processSpy.exit is not a function

// in ProcessSpy
    this.exit = function (code) {
        this.hasExitedWithCode = 0;
    };
// =&gt; PASS

    this.testHasExitedWithCode_isOne_afterExitOneCall = function () {
        processSpy.exit(1);
        t.assertEqual(1, processSpy.hasExitedWithCode);
    };
// =&gt; Error: Expected to equal 1, but got: 0

// in ProcessSpy
    this.exit = function (code) {
        this.hasExitedWithCode = code;
        // changed 0 to code      ^here^
    };
</code></pre>

<p>I think we have finished test-driving the functionality of <code>ProcessSpy</code>. It is time to get back to our failing test for a failure resulting in an exit with code 1. When we run this test suite, we are getting the following error message: <code>Error: Expected to equal 1, but got: null</code>.&lsquo; To pass this test, we will need to store the fact that we had a failure somewhere and at the end of the test suite run we can trigger exit with code 1 or 0, respectively. We could pass around a <code>status</code> object with boolean property <code>status.failed</code> and set it to <code>true</code> in our <code>catch</code> block:</p>

<pre><code class="javascript">    } catch (error) {
        if (!silenceFailures) console.log(error);
        status.failed = true;
    }
</code></pre>

<p>And at the end of <code>runTestSuite</code> function we could call <code>process.exit(1)</code> if <code>status.failed</code> was <code>true</code>:</p>

<pre><code class="javascript">function runTestSuite(testSuiteConstructor, options) {
    // ...

    if (status.failed) {
        process.exit(1);
    }
}
</code></pre>

<p>While this works (as in &ldquo;tests pass after providing <code>fakeProcess</code> where needed for nested failing <code>runTestSuite</code> calls&rdquo;) state changes in this code are starting to be hard to follow and function signatures remind me of some horror movie:</p>

<pre><code class="javascript">function getTestSuiteName(testSuiteConstructor, testSuitePrototype)
function runTest(testSuite, testName, silenceFailures, status)
function handleTest(reporter, testName, testSuiteConstructor, silenceFailures, status)
function runAllTests(reporter, testSuitePrototype, testSuiteConstructor, silenceFailures, status)
</code></pre>

<p>These signatures smell like objects are hiding there in these functions. Let&rsquo;s find them!</p>

<h2>Quest for hidden objects</h2>

<p>First, let&rsquo;s extract the method object from the function <code>runTestSuite</code>. We will give it a name <code>TestSuiteRunContext</code>:</p>

<pre><code class="javascript">function TestSuiteRunContext(testSuiteConstructor, options) {
    options = options || {};
    var reporter = options.reporter || new SimpleReporter();
    var process = options.process || global.process;
    var silenceFailures = options.silenceFailures || false;

    var status = {failed: false};

    var testSuitePrototype = createTestSuite(testSuiteConstructor);

    this.invoke = function () {
        reporter.reportTestSuite(
            getTestSuiteName(testSuiteConstructor, testSuitePrototype)
        );

        runAllTests(
            reporter,
            testSuitePrototype,
            testSuiteConstructor,
            silenceFailures,
            status
        );

        if (status.failed) {
            process.exit(1);
        }
    };
}

function runTestSuite(testSuiteConstructor, options) {
    new TestSuiteRunContext(testSuiteConstructor, options).invoke();
}
</code></pre>

<p>Now, if we were to move function <code>runAllTests</code> inside of this class, we would not need all these arguments (and all other functions we call):</p>

<pre><code class="javascript">    this.invoke = function () {
        reportTestSuite();
        runAllTests();
        finishTestRun();
    };

    function reportTestSuite() {
        reporter.reportTestSuite(getTestSuiteName());
    }

    function getTestSuiteName() {
        if (typeof(createTestSuite().getTestSuiteName) !== "function") {
            return testSuiteConstructor.name;
        }

        return createTestSuite().getTestSuiteName();
    }

    function createTestSuite() {
        return new testSuiteConstructor(assertions);
    }

    function runAllTests() {
        for (var testName in createTestSuite()) {
            if (testName.match(/^test/)) {
                handleTest(testName);
            }
        }
    }

    function handleTest(testName) {
        reportTest(testName);
        runTest(createTestSuite(), testName);
    }

    function reportTest(testName) {
        reporter.reportTest(testName);
    }

    function runTest(testSuite, testName) {
        try {
            testSuite[testName]();
        } catch (error) {
            if (!silenceFailures) console.log(error);
            status.failed = true;
        }
    }

    function finishTestRun() {
        if (status.failed) {
            process.exit(1);
        }
    }
</code></pre>

<p>It already looks very nice. The only thing that I do not like about this object yet is that it has stateful properties and stateless properties. I like to have my objects separated by this concern. Let&rsquo;s extract <code>status</code> mutable property as a proper <code>TestSuiteRunStatus</code> object:</p>

<pre><code class="javascript">function TestSuiteRunStatus() {
    var failed = false;

    this.markAsFailed = function () {
        failed = true;
    };

    this.hasFailed = function () {
        return failed;
    };
}

function TestSuiteRunContext(testSuiteConstructor, options) {
  // ...
  var status = new TestSuiteRunStatus();

  // ...
  function runTest(testSuite, testName) {
        try {
            testSuite[testName]();
        } catch (error) {
            if (!silenceFailures) console.log(error);
            status.markAsFailed();
        }
    }

    function finishTestRun() {
        if (status.hasFailed()) {
            process.exit(1);
        }
    }
}
</code></pre>

<p>I think we have finished the refactoring. Now we should verify that test suite exits with the code 0 when everything passes:</p>

<pre><code class="javascript">    this.testItExitsWithProcessCodeZero_onSuccess = function () {
        runTestSuite(function (t) {
            this.testFailure = function () {
                t.assertTrue(true);
            };
        }, {process: processSpy, silenceFailures: true});

        t.assertEqual(0, processSpy.hasExitedWithCode);
    };
// =&gt; Error: Expected to equal 0, but got: null

    function finishTestRun() {
        if (status.hasFailed()) return process.exit(1);
        process.exit(0);
    }
// =&gt; PASS
</code></pre>

<h2>Bottom Line</h2>

<p>I think we have finished implementing exit code reporting. The code can be found here: <a href="https://github.com/waterlink/BuildYourOwnTestingFrameworkPart5">https://github.com/waterlink/BuildYourOwnTestingFrameworkPart5</a></p>

<p>There is still a lot to go through. In a few next episodes we will:</p>

<ul>
<li>Report OK and FAIL for each test;</li>
<li>Output carefully formatted failures to the STDERR;</li>
<li>Enable our testing framework to run multiple test suite files at once;</li>
<li>Enable our testing framework to run in a browser (it is javascript after all).</li>
</ul>


<p>Stay tuned!</p>

<h2>Thanks</h2>

<p>Thank you for reading, my dear reader. If you liked it, please share this article on social networks and follow me on Twitter: <a href="https://twitter.com/waterlink000">@waterlink000</a>.</p>

<p>If you have any questions or feedback for me, don’t hesitate to reach me out on Twitter: <a href="https://twitter.com/waterlink000">@waterlink000</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HighScore Kata]]></title>
    <link href="http://www.tddfellow.com/blog/2016/09/30/highscore-kata/"/>
    <updated>2016-09-30T18:23:21+02:00</updated>
    <id>http://www.tddfellow.com/blog/2016/09/30/highscore-kata</id>
    <content type="html"><![CDATA[<p>Hello, everyone. Today we will take a look into a little problem involving high scores in some sort of game. The game has only one high score and when current game&rsquo;s score exceeds that number, it gets updated. Example acceptance test would read like this:</p>

<p><strong>Given</strong> high score is <code>174</code><br/>
<strong>When</strong> player scores <code>191</code><br/>
<strong>Then</strong> high score is <code>191</code></p>

<!--more-->


<p>Current implementation stores high score in the web browser&rsquo;s local storage. This detail does not change the purpose of this Kata very much since any other platform and language can have its own analog of local storage (file system, in-memory or local database, application settings, etc.). <code>HighScore</code> object looks like this:</p>

<pre><code class="javascript">function Highscore() {
    // initially, load high score value from the local storage
    this.load();
}

Highscore.prototype.updateHighscore = function (score) {
    // check if we need to update high score
    if (score &gt; this.highscore) {
        this.highscore = score;
        this.save();
    }

    // render the high score in the
    // id="highscore" element in the browser
    $("#highscore").text("HIGHSCORE: " + this.highscore);
};

Highscore.prototype.load = function () {
    // localStorage is storing everything as strings,
    // so we need to convert it to number
    this.highscore = parseFloat(localStorage.highscore);
};

Highscore.prototype.save = function () {
    localStorage.highscore = this.highscore;
};
</code></pre>

<p>Task for the Kata:</p>

<ul>
<li>Write tests for this class.</li>
<li>Fix the bug: when a game is launched on the new client (without the high score stored), it renders <code>HIGHSCORE: NaN</code>. (<code>NaN</code> is javascript&rsquo;s abbreviation for &ldquo;not a number&rdquo;). <code>parseFloat</code> most probably is a culprit for this.</li>
<li>Extract storing mechanisms, so that class can be re-used with different storage mechanisms (for example local database, or external REST API).</li>
<li>Make all tests runnable outside of the context of the browser (for example, on <code>nodejs</code>).</li>
</ul>


<p>The focus of this Kata is on architectural boundaries, that this little innocent class spans.</p>

<p>Questions to ask yourself:</p>

<ul>
<li>How much distinct responsibilities this class has?</li>
<li>What architectural boundaries should I draw through this class?

<ul>
<li>How do I split this class according to these boundaries?</li>
</ul>
</li>
<li>What is the easiest way and what is the proper way to make tests runnable outside of the context of the browser?</li>
<li>How would this code look like in different kinds of languages? (static, dynamic, object-oriented, functional, strong-typed, etc.).

<ul>
<li>And how the solution for the Kata will look like for these?</li>
</ul>
</li>
</ul>


<p>Next time we will take a look at one possible solution for this Kata. Try to solve it on your own, my dear reader, and please share the code and insights!</p>

<h2>Thanks</h2>

<p>Thank you for reading, my dear reader. If you liked it, please share this article on social networks and follow me on twitter: <a href="https://twitter.com/waterlink000">@waterlink000</a>.</p>

<p>If you have any questions or feedback for me, don&rsquo;t hesitate to reach me out on Twitter: <a href="https://twitter.com/waterlink000">@waterlink000</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Meet Duck Type]]></title>
    <link href="http://www.tddfellow.com/blog/2016/09/18/meet-duck-type/"/>
    <updated>2016-09-18T16:37:33+02:00</updated>
    <id>http://www.tddfellow.com/blog/2016/09/18/meet-duck-type</id>
    <content type="html"><![CDATA[<h2>Duck type</h2>

<p>Duck type is the concept in the domain of the type safety that represents objects, that pass a so-called &ldquo;Duck Test&rdquo;:</p>

<blockquote><p>If it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck.</p></blockquote>

<p>In terms of programming language, it might look like this:</p>

<pre><code class="javascript">function Duck() {
    this.swim = function (coordinates) { ... };
    this.quack = function (sentence) { ... };
}

function RoboDuck() {
    this.swim = function (coordinates) { ... };
    this.quack = function (sentence) { ... };
}

// .. and so on ..
</code></pre>

<!--more-->


<p>The point is that the public interface has methods <code>swim()</code> and <code>quack()</code>. This is how you identify the duck in a programming language. This concept is very similar to the concept of the <code>interface</code> in programming languages that have one, but it is not enforced in any way by the programming language.</p>

<p>Duck typing is mostly natural in dynamic languages, where it is possible to send any message to any object and the check if that is something possible will happen at runtime. In static languages, it is still possible to use duck typing via some sort of Reflection.</p>

<h3>Contract test for duck types</h3>

<p>In a dynamic language, it is important to make it obvious, that something is implementing certain duck type by writing one test suite for all implementers and executing it against them. For example:</p>

<pre><code class="javascript">[Duck, RoboDuck].forEach(function (duckType) {
    runTestSuite(function (t) {
        this.testItSwimsLikeADuck = function () {
            var duck = new duckType();
            duck.swim({x: 5, y: 7});
            t.assertThat(duck).swamTo({x: 5, y: 7});
        };

        this.testItQuacksLikeADuck = function () {
            var duck = new duckType();
            duck.quack("hello world");
            t.assertThat(duck).quacked("hello world");
        };
    });
});
</code></pre>

<p>This test suite has to go only through the Duck type public interface. If it is not possible to test behavior through it, one should test at least the function signatures, for example:</p>

<pre><code class="javascript">this.testItSwimsLikeADuck = function () {
    var duck = new duckType();
    var swim = duck.swim;
    t.assertEqual("function", typeof(swim));
    t.assertEqual(1, swim.length);
};
</code></pre>

<p>Not doing contract tests for your ducks may result in a passing test suite and broken production code. For example, when one duck and its test suite have been updated, but others haven&rsquo;t.</p>

<h2>Thanks</h2>

<p>Thank you for reading, my dear reader. If you liked it, please share this article on social networks and follow me on twitter: <a href="https://twitter.com/waterlink000">@waterlink000</a>.</p>

<p>If you have any questions or feedback for me, don&rsquo;t hesitate to reach me out on Twitter: <a href="https://twitter.com/waterlink000">@waterlink000</a>.</p>
]]></content>
  </entry>
  
</feed>
